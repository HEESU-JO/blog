---
title: "Regularization(정규화) 이해하기 — 왜 필요하고 어떻게 작동할까?"
date: 2025-01-01
categories: [machine-learning]
tags: [regularization, l1, l2, elastic-net, overfitting]
---

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$','$$'], ['\\[','\\]']]
  }
};
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

## 1️⃣ 왜 규제가 필요한가?

모델을 훈련하다 보면 종종 훈련 데이터에 너무 과하게 맞춰지는 현상, 즉 **overfitting**이 발생한다.

“데이터를 잘 맞추면 좋은 거 아닌가?”라고 생각할 수 있지만, 꼭 그렇지는 않다.

예를 들어, 어떤 학생이 기출문제만 완벽하게 외워서 전부 맞췄다고 하자.  
하지만 실제 시험에서 문제 유형이 조금만 달라지면 오답을 낸다면, 그 학생이 진짜 실력이 있다고 말하기 어렵다.

머신러닝 모델도 마찬가지다.  
훈련 데이터에 지나치게 맞춰지면 **처음 보는 데이터(test data)**에서는 성능이 떨어진다.  
이게 바로 오버피팅이다.

### 🔍 Overfitting이 주로 일어나는 상황

| 상황 | 설명 |
|------|------|
| 모델이 너무 복잡함 | 파라미터 수가 많거나 자유도가 높음 |
| 데이터가 적음     | 다양한 패턴을 학습하지 못하고 노이즈까지 외움 |
| 노이즈가 많음     | 불필요한 변동까지 패턴으로 착각함 |

---

## 2️⃣ Regularization

머신러닝에서 손실 함수는 기본적으로 다음과 같다:

$$
L(w) = \|y - Xw\|_2^2 + \lambda C(w)
$$

- **오차항**: 예측값과 실제값의 차이  
- **규제항**: 모델이 복잡해지는 것을 벌점으로 부여  
- **λ (람다)**: 규제의 강도

규제항 $C(w)$ 가 무엇이냐에 따라 L1, L2, Elastic Net으로 나뉜다.

---

### ● L1(Lasso) Regularization

$$
L_{L1}(w) = \|y - Xw\|_2^2 + \lambda \|w\|_1
$$

- L1 규제는 **가중치 절댓값의 합**을 벌점으로 준다.  
- 효과: **희소성(sparsity)** → 일부 가중치를 0으로 만들어 피처 선택 효과 발생

---

### ● L2(Ridge) Regularization

$$
L_{L2}(w) = \|y - Xw\|_2^2 + \lambda \|w\|_2^2
$$

- L2 규제는 **가중치 제곱의 합**을 벌점으로 준다.  
- 효과: **가중치가 지나치게 커지는 것을 억제** (스무딩 효과)

---

### ● Elastic Net (L1 + L2)

$$
L_{EN}(w) = \|y - Xw\|_2^2 + \lambda_1 \|w\|_1 + \lambda_2 \|w\|_2^2
$$

- L1의 희소성과 L2의 안정성을 결합한 방식  


---

## 3️⃣ 왜 오차항 + 규제항이 Overfitting을 막는가?

모델은 학습할 때 **오차(Error)를 최소화하는 방향**으로 가중치 $w$ 를 조정한다.

그런데 오버피팅이 일어나면  
→ **노이즈까지 학습**하면서  
→ 가중치(특히 특정 피처의 가중치)가 지나치게 커지게 된다.

예: L2 규제를 포함한 손실 함수

$$
L_{L2}(w) = \|y - Xw\|_2^2 + \lambda \|w\|_2^2
$$

여기서 가중치가 커질수록:

- 규제항 $\|w\|_2^2$ 도 커지고  
- 전체 손실값도 증가한다  

즉, 모델이 **큰 가중치를 가지면 벌점을 받도록 설계**되어 있다.

➡️ 그래서 모델은 **가중치를 과도하게 키우지 못하고**,  
➡️ 자연스럽게 **복잡도를 제한**하며,  
➡️ 노이즈까지 외우는 오버피팅을 방지한다.

---

## 4️⃣ λ(람다)에 따른 변화

- λ가 **작음** → 규제 약함 → 모델이 자유롭게 학습 → 오버피팅 위험 ↑  
- λ가 **큼** → 규제 강함 → 모델 단순해짐 → 오버피팅 ↓  
- λ가 **너무 크면** → underfitting 발생

✔ 결론: 적절한 λ를 찾는 것이 핵심!  
(보통 cross-validation으로 최적 λ를 찾는다.)

---
